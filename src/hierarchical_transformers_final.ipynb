{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPcWdnAMly5FjaYx8G6lUjy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"dd2f640298b140f8b78dc02c50599b35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f0e18d98b224fe7aa2b14a6d58c5fed","IPY_MODEL_eeeccaa8cd954c5387b5211aa589d6a7","IPY_MODEL_9d4f1513cb224f2999165a16fe6825c6"],"layout":"IPY_MODEL_ae6cc7aca51c453287a9ce5e616cd4f6"}},"0f0e18d98b224fe7aa2b14a6d58c5fed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77627206780249ef82443b4a50d738bc","placeholder":"​","style":"IPY_MODEL_600aa73f9eac40f3bc564c87609d4d65","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"eeeccaa8cd954c5387b5211aa589d6a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40d468e00a9848daa2ba50d5e9ab3578","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05f28d4d9994491f99fb08f7c70344f8","value":213450}},"9d4f1513cb224f2999165a16fe6825c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb8c9328aad8462980e834076aa4ff27","placeholder":"​","style":"IPY_MODEL_a8d9b5e0b57342ecb118567852f42750","value":" 213k/213k [00:00&lt;00:00, 505kB/s]"}},"ae6cc7aca51c453287a9ce5e616cd4f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77627206780249ef82443b4a50d738bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"600aa73f9eac40f3bc564c87609d4d65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40d468e00a9848daa2ba50d5e9ab3578":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05f28d4d9994491f99fb08f7c70344f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb8c9328aad8462980e834076aa4ff27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d9b5e0b57342ecb118567852f42750":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd219f27fab247ebbb5f38ea6faf5c17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b938df240274dafb82ad004e01d8950","IPY_MODEL_21399ee13e784625ba0cc55687b8ed7f","IPY_MODEL_eba2b1104d41408eb0d7542efb252c77"],"layout":"IPY_MODEL_e7c9d1e6b6b2475ca0cb76e311764c53"}},"4b938df240274dafb82ad004e01d8950":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26e372f1691447e2bc2b83f0afaccc46","placeholder":"​","style":"IPY_MODEL_8156671ffa6444afb9c58d0f649c7fca","value":"Downloading (…)okenizer_config.json: 100%"}},"21399ee13e784625ba0cc55687b8ed7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a503d0911c1f4c8ca2824895591814c8","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f09db750d734d8b82f91947f1303089","value":29}},"eba2b1104d41408eb0d7542efb252c77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b4cd142ab7c4347bb3d967d6ca4954d","placeholder":"​","style":"IPY_MODEL_0bea82b80d924552bdca41b5541cf3c8","value":" 29.0/29.0 [00:00&lt;00:00, 1.89kB/s]"}},"e7c9d1e6b6b2475ca0cb76e311764c53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e372f1691447e2bc2b83f0afaccc46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8156671ffa6444afb9c58d0f649c7fca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a503d0911c1f4c8ca2824895591814c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f09db750d734d8b82f91947f1303089":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b4cd142ab7c4347bb3d967d6ca4954d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bea82b80d924552bdca41b5541cf3c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c29bb70ab5fa446b81cef6e97182745b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fe395ca8a72481f9dab5155ddc76349","IPY_MODEL_6188462d6fbe461aa51fc910c4f80759","IPY_MODEL_b31d49022ea8449fbcf3598ead797ee2"],"layout":"IPY_MODEL_81a62656c24d482ea9ad4d038d2367dd"}},"6fe395ca8a72481f9dab5155ddc76349":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07dc5240742e414cb91fa63704afbd69","placeholder":"​","style":"IPY_MODEL_854ee5ee685e4e5eb06a23d21e4d31c1","value":"Downloading (…)lve/main/config.json: 100%"}},"6188462d6fbe461aa51fc910c4f80759":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bf325c0519e4f57b6dd9c8182e488ed","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9dd5c5c4fd3044d0b5377fb41bfd485a","value":570}},"b31d49022ea8449fbcf3598ead797ee2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3efc0a7d37e4d4d8e2bf9b6791879a1","placeholder":"​","style":"IPY_MODEL_f86427a11d034161a687151759830599","value":" 570/570 [00:00&lt;00:00, 46.1kB/s]"}},"81a62656c24d482ea9ad4d038d2367dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07dc5240742e414cb91fa63704afbd69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"854ee5ee685e4e5eb06a23d21e4d31c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bf325c0519e4f57b6dd9c8182e488ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd5c5c4fd3044d0b5377fb41bfd485a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3efc0a7d37e4d4d8e2bf9b6791879a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f86427a11d034161a687151759830599":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d0add3483b5489a99a3773e061a9456":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25f7baa040aa47b28e75dbac21b9be25","IPY_MODEL_da51681035f644608c11ad180a491325","IPY_MODEL_e36a5b6707014804815a5ff86b2500f4"],"layout":"IPY_MODEL_e6101603eed54247984260106677b393"}},"25f7baa040aa47b28e75dbac21b9be25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a38114d30747d28f19071a6d163cb7","placeholder":"​","style":"IPY_MODEL_a4e75aeba33c45378b326ac11d9baac6","value":"Downloading pytorch_model.bin: 100%"}},"da51681035f644608c11ad180a491325":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5399c1fcd87f416793abf8c32f484599","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_913fbd3db5044595950da737acdf9e07","value":435779157}},"e36a5b6707014804815a5ff86b2500f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9982eed0fb994719a7346fe1a0a746f1","placeholder":"​","style":"IPY_MODEL_613a396aac2443518f4fcf9ec02f05f8","value":" 436M/436M [00:01&lt;00:00, 410MB/s]"}},"e6101603eed54247984260106677b393":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42a38114d30747d28f19071a6d163cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4e75aeba33c45378b326ac11d9baac6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5399c1fcd87f416793abf8c32f484599":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"913fbd3db5044595950da737acdf9e07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9982eed0fb994719a7346fe1a0a746f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613a396aac2443518f4fcf9ec02f05f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# HIERARCHICAL TRANSFORMERS PERSONALITY CLASSIFICATION"],"metadata":{"id":"j1WzkYStB6ec"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"tFgnCNm8CQr_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685778837464,"user_tz":-120,"elapsed":12954,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"c5f28c3f-28ab-4b3f-bb30-3b9b87c69d2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"]}]},{"cell_type":"code","source":["import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, f1_score\n","from collections import defaultdict\n","from textwrap import wrap\n","\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F"],"metadata":{"id":"6JaTXTpYCGdx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Setting up"],"metadata":{"id":"i_Ozx1_hFSnm"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQ2Ksbq6DcP4","executionInfo":{"status":"ok","timestamp":1685778974700,"user_tz":-120,"elapsed":23954,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"22a30487-2580-4aed-ebc8-d5a1cda51e8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n","df = pd.read_excel('essays.xlsx')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"id":"QJT54uPqDdkN","executionInfo":{"status":"ok","timestamp":1685779028819,"user_tz":-120,"elapsed":54125,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"a6a01cd4-bf4f-4ea5-b89f-38ce9184abbf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6bd19591-87bc-4aa8-8ec3-772cad5e079b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6bd19591-87bc-4aa8-8ec3-772cad5e079b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving essays.xlsx to essays.xlsx\n"]},{"output_type":"execute_result","data":{"text/plain":["           #AUTHID                                               TEXT cEXT  \\\n","0  1997_504851.txt  Well, right now I just woke up from a mid-day ...    n   \n","1  1997_605191.txt  Well, here we go with the stream of consciousn...    n   \n","2  1997_687252.txt  An open keyboard and buttons to push. The thin...    n   \n","3  1997_568848.txt  I can't believe it!  It's really happening!  M...    y   \n","4  1997_688160.txt  Well, here I go with the good old stream of co...    y   \n","\n","  cNEU cAGR cCON cOPN  \n","0    y    y    n    y  \n","1    n    y    n    n  \n","2    y    n    y    y  \n","3    n    y    y    n  \n","4    n    y    n    y  "],"text/html":["\n","  <div id=\"df-38597882-de5d-4fe7-96df-7b11dc4a7f8d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>#AUTHID</th>\n","      <th>TEXT</th>\n","      <th>cEXT</th>\n","      <th>cNEU</th>\n","      <th>cAGR</th>\n","      <th>cCON</th>\n","      <th>cOPN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1997_504851.txt</td>\n","      <td>Well, right now I just woke up from a mid-day ...</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>y</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1997_605191.txt</td>\n","      <td>Well, here we go with the stream of consciousn...</td>\n","      <td>n</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1997_687252.txt</td>\n","      <td>An open keyboard and buttons to push. The thin...</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>y</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1997_568848.txt</td>\n","      <td>I can't believe it!  It's really happening!  M...</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>y</td>\n","      <td>n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1997_688160.txt</td>\n","      <td>Well, here I go with the good old stream of co...</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>y</td>\n","      <td>n</td>\n","      <td>y</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38597882-de5d-4fe7-96df-7b11dc4a7f8d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-38597882-de5d-4fe7-96df-7b11dc4a7f8d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-38597882-de5d-4fe7-96df-7b11dc4a7f8d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["personalities=[\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n","\n","for personality in personalities:\n","    df[personality].replace([\"y\", \"n\"], [1, 0], inplace=True)\n","\n","df.drop(columns='#AUTHID', inplace=True)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"fYVZbgS7FM5f","executionInfo":{"status":"ok","timestamp":1685779151032,"user_tz":-120,"elapsed":500,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"1f3f5af0-0b0a-46d3-80e5-b62c8e290593"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                TEXT  cEXT  cNEU  cAGR  cCON  \\\n","0  Well, right now I just woke up from a mid-day ...     0     1     1     0   \n","1  Well, here we go with the stream of consciousn...     0     0     1     0   \n","2  An open keyboard and buttons to push. The thin...     0     1     0     1   \n","3  I can't believe it!  It's really happening!  M...     1     0     1     1   \n","4  Well, here I go with the good old stream of co...     1     0     1     0   \n","\n","   cOPN  \n","0     1  \n","1     0  \n","2     1  \n","3     0  \n","4     1  "],"text/html":["\n","  <div id=\"df-a78d8b29-049d-4926-9316-4f73f0959f92\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TEXT</th>\n","      <th>cEXT</th>\n","      <th>cNEU</th>\n","      <th>cAGR</th>\n","      <th>cCON</th>\n","      <th>cOPN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Well, right now I just woke up from a mid-day ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Well, here we go with the stream of consciousn...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>An open keyboard and buttons to push. The thin...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I can't believe it!  It's really happening!  M...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Well, here I go with the good old stream of co...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a78d8b29-049d-4926-9316-4f73f0959f92')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a78d8b29-049d-4926-9316-4f73f0959f92 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a78d8b29-049d-4926-9316-4f73f0959f92');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#Split each text longer than 200 words into multiple texts with an overlap of 50 words\n","import math\n","\n","def split_text_into_segments(text, segment_length, overlap):\n","    words = text.split()\n","    segments = []\n","    for i in range(0, len(words), segment_length - overlap):\n","        segment = \" \".join(words[i:i+segment_length])\n","        segments.append(segment)\n","    return segments\n","\n","def split_long_texts(df, max_length, overlap):\n","    new_rows = []\n","    for index, row in df.iterrows():\n","        text = row['TEXT']\n","        atr_cols = row[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']]\n","        segments = split_text_into_segments(text, max_length, overlap)\n","        for segment in segments:\n","            new_row = {'text': segment}\n","            new_row.update(atr_cols)\n","            new_rows.append(new_row)\n","    new_df = pd.DataFrame(new_rows)\n","    return new_df\n","\n","max_length = 200  # Maximum length of each segment in words\n","overlap = 50  # Number of overlapping words between segments\n","\n","split_df = split_long_texts(df, max_length, overlap)\n","split_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uqaN5k1IHZf1","executionInfo":{"status":"ok","timestamp":1685779154084,"user_tz":-120,"elapsed":1383,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"1fd1bc97-517c-438d-9393-ced771e067ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  cEXT  cNEU  cAGR  cCON  \\\n","0  Well, right now I just woke up from a mid-day ...     0     1     1     0   \n","1  to correct the past, but I don't really know h...     0     1     1     0   \n","2  I don't have a worry in the world. The only th...     0     1     1     0   \n","3  keep my priorities straight. Living at home, I...     0     1     1     0   \n","4  put more pressure on me to do better in school...     0     1     1     0   \n","\n","   cOPN  \n","0     1  \n","1     1  \n","2     1  \n","3     1  \n","4     1  "],"text/html":["\n","  <div id=\"df-30b1ef32-1cd9-4ee3-8000-f5b8d552a061\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>cEXT</th>\n","      <th>cNEU</th>\n","      <th>cAGR</th>\n","      <th>cCON</th>\n","      <th>cOPN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Well, right now I just woke up from a mid-day ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>to correct the past, but I don't really know h...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I don't have a worry in the world. The only th...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>keep my priorities straight. Living at home, I...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>put more pressure on me to do better in school...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30b1ef32-1cd9-4ee3-8000-f5b8d552a061')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-30b1ef32-1cd9-4ee3-8000-f5b8d552a061 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-30b1ef32-1cd9-4ee3-8000-f5b8d552a061');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["split_df.info()\n","split_df.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":526},"id":"efLZGA6aKBJu","executionInfo":{"status":"ok","timestamp":1685779154581,"user_tz":-120,"elapsed":12,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"5e12ec12-74e3-469a-8c2f-4bc91453e9d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 11960 entries, 0 to 11959\n","Data columns (total 6 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    11960 non-null  object\n"," 1   cEXT    11960 non-null  int64 \n"," 2   cNEU    11960 non-null  int64 \n"," 3   cAGR    11960 non-null  int64 \n"," 4   cCON    11960 non-null  int64 \n"," 5   cOPN    11960 non-null  int64 \n","dtypes: int64(5), object(1)\n","memory usage: 560.8+ KB\n"]},{"output_type":"execute_result","data":{"text/plain":["               cEXT          cNEU          cAGR          cCON          cOPN\n","count  11960.000000  11960.000000  11960.000000  11960.000000  11960.000000\n","mean       0.517559      0.508278      0.529599      0.511957      0.521572\n","std        0.499712      0.499952      0.499144      0.499878      0.499555\n","min        0.000000      0.000000      0.000000      0.000000      0.000000\n","25%        0.000000      0.000000      0.000000      0.000000      0.000000\n","50%        1.000000      1.000000      1.000000      1.000000      1.000000\n","75%        1.000000      1.000000      1.000000      1.000000      1.000000\n","max        1.000000      1.000000      1.000000      1.000000      1.000000"],"text/html":["\n","  <div id=\"df-09d1b5bf-2f6a-4b78-96df-a81cd2b98a5f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cEXT</th>\n","      <th>cNEU</th>\n","      <th>cAGR</th>\n","      <th>cCON</th>\n","      <th>cOPN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>11960.000000</td>\n","      <td>11960.000000</td>\n","      <td>11960.000000</td>\n","      <td>11960.000000</td>\n","      <td>11960.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.517559</td>\n","      <td>0.508278</td>\n","      <td>0.529599</td>\n","      <td>0.511957</td>\n","      <td>0.521572</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.499712</td>\n","      <td>0.499952</td>\n","      <td>0.499144</td>\n","      <td>0.499878</td>\n","      <td>0.499555</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09d1b5bf-2f6a-4b78-96df-a81cd2b98a5f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-09d1b5bf-2f6a-4b78-96df-a81cd2b98a5f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-09d1b5bf-2f6a-4b78-96df-a81cd2b98a5f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["RANDOM_SEED = 72\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSLLdw2xCgQM","executionInfo":{"status":"ok","timestamp":1685779156117,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"64b03485-1c81-43ac-b8a2-c6264fc34af9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd66755dfd0>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZJ5f__KD-t1","executionInfo":{"status":"ok","timestamp":1685779157358,"user_tz":-120,"elapsed":3,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"21b3a383-affc-42e4-d62d-6717c3370a28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"metadata":{"id":"fREGtlhqECY-","executionInfo":{"status":"ok","timestamp":1685779160369,"user_tz":-120,"elapsed":2542,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["dd2f640298b140f8b78dc02c50599b35","0f0e18d98b224fe7aa2b14a6d58c5fed","eeeccaa8cd954c5387b5211aa589d6a7","9d4f1513cb224f2999165a16fe6825c6","ae6cc7aca51c453287a9ce5e616cd4f6","77627206780249ef82443b4a50d738bc","600aa73f9eac40f3bc564c87609d4d65","40d468e00a9848daa2ba50d5e9ab3578","05f28d4d9994491f99fb08f7c70344f8","cb8c9328aad8462980e834076aa4ff27","a8d9b5e0b57342ecb118567852f42750","fd219f27fab247ebbb5f38ea6faf5c17","4b938df240274dafb82ad004e01d8950","21399ee13e784625ba0cc55687b8ed7f","eba2b1104d41408eb0d7542efb252c77","e7c9d1e6b6b2475ca0cb76e311764c53","26e372f1691447e2bc2b83f0afaccc46","8156671ffa6444afb9c58d0f649c7fca","a503d0911c1f4c8ca2824895591814c8","6f09db750d734d8b82f91947f1303089","8b4cd142ab7c4347bb3d967d6ca4954d","0bea82b80d924552bdca41b5541cf3c8","c29bb70ab5fa446b81cef6e97182745b","6fe395ca8a72481f9dab5155ddc76349","6188462d6fbe461aa51fc910c4f80759","b31d49022ea8449fbcf3598ead797ee2","81a62656c24d482ea9ad4d038d2367dd","07dc5240742e414cb91fa63704afbd69","854ee5ee685e4e5eb06a23d21e4d31c1","8bf325c0519e4f57b6dd9c8182e488ed","9dd5c5c4fd3044d0b5377fb41bfd485a","c3efc0a7d37e4d4d8e2bf9b6791879a1","f86427a11d034161a687151759830599"]},"outputId":"e1b36b48-fd21-4365-bc22-72b9a0a3b315"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd2f640298b140f8b78dc02c50599b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd219f27fab247ebbb5f38ea6faf5c17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29bb70ab5fa446b81cef6e97182745b"}},"metadata":{}}]},{"cell_type":"code","source":["#An example of what will be performed on big scale\n","sample_txt='Personal Health Record (Extract)\\nCreated on October 24, 2019\\nPatient\\nSteven Fuerst\\nBirthdate\\nDecember 10, 1979\\nRace\\nInformation not\\navailable'\n","\n","tokens = tokenizer.tokenize(sample_txt)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","encoding = tokenizer.encode_plus(\n","                  sample_txt,\n","                  max_length=128,\n","                  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","                  return_token_type_ids=False,\n","                  pad_to_max_length=True,\n","                  return_attention_mask=True,\n","                  return_tensors='pt',  # Return PyTorch tensors\n","                )\n","print(encoding)              "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDHxTkeMFmWZ","executionInfo":{"status":"ok","timestamp":1685779160369,"user_tz":-120,"elapsed":4,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"36ae9728-634c-4a0f-d19f-7e08b2df73ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[  101, 13907,  3225,  7992,   113, 18684,  5822,   114, 25423,  1113,\n","          1357,  1572,   117, 10351,  7195,  9080,  6536, 14763,  1468,  1204,\n","         20729,  9216,  1382,  1275,   117,  2333,  6398,  4219,  1136,  1907,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#### Datasets"],"metadata":{"id":"szlR1YlPazqo"}},{"cell_type":"code","source":["class EssayDataset(Dataset):\n","    \n","    def __init__(self, doc, targets, tokenizer, max_len):\n","        self.doc = doc\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    \n","    def __len__(self):\n","        return len(self.doc)\n","  \n","    def __getitem__(self, item):\n","        doc = str(self.doc[item])\n","        target = self.targets[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","                    doc,\n","                    add_special_tokens=True,\n","                    max_length=self.max_len,\n","                    return_token_type_ids=False,\n","                    pad_to_max_length=True,\n","                    return_attention_mask=True,\n","                    return_tensors='pt',\n","                    )\n","\n","        return {\n","            'doc_text': doc,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","            }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size, personality):\n","    ds = EssayDataset(\n","        doc=df.text.to_numpy(),\n","        targets=df[personality].to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","      )\n","    \n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=2\n","        )\n","    "],"metadata":{"id":"iOWm58p1dKtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train, df_test = train_test_split(split_df, test_size=0.1, random_state=RANDOM_SEED)\n","df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"],"metadata":{"id":"V-f7JLy2ayQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 256                                                       #MORA LI BITI POTENCIJA OD 2\n","BATCH_SIZE = 16\n","\n","#personalities=[\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n","\n","#cEXT\n","train_data_loader_ext = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, \"cEXT\")\n","val_data_loader_ext = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE, \"cEXT\")\n","test_data_loader_ext = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, \"cEXT\")\n","\n","#cNEU\n","train_data_loader_neu = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, \"cNEU\")\n","val_data_loader_neu = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE, \"cNEU\")\n","test_data_loader_neu = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, \"cNEU\")\n","\n","#cAGR\n","train_data_loader_agr = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, \"cAGR\")\n","val_data_loader_agr = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE, \"cAGR\")\n","test_data_loader_agr = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, \"cAGR\")\n","\n","#cCON\n","train_data_loader_con = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, \"cCON\")\n","val_data_loader_con = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE, \"cCON\")\n","test_data_loader_con = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, \"cCON\")\n","\n","#cOPN\n","train_data_loader_opn = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, \"cOPN\")\n","val_data_loader_opn = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE, \"cOPN\")\n","test_data_loader_opn = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, \"cOPN\")"],"metadata":{"id":"kOc3BrIRdJvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Definition"],"metadata":{"id":"6KlfnCWaiIA6"}},{"cell_type":"code","source":["class PersonalityClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(PersonalityClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _, pooled_output = self.bert(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask,\n","          return_dict=False\n","        )\n","        \n","        output = self.drop(pooled_output)\n","        return self.out(output)"],"metadata":{"id":"RocsbHkgiwdQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Extraversion"],"metadata":{"id":"WZTZmjMqHejX"}},{"cell_type":"code","source":["data_ext = next(iter(train_data_loader_ext))\n","\n","model_ext = PersonalityClassifier(2)  #2 is number of classes (binary classifier for a personality)\n","model_ext = model_ext.to(device)\n","\n","input_ids = data_ext['input_ids'].to(device)\n","attention_mask = data_ext['attention_mask'].to(device)"],"metadata":{"id":"EV8GEuM9fH54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685752095165,"user_tz":-120,"elapsed":3367,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"d51c7057-d59b-4d59-a58d-8dc932a030b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["##### Neuroticism"],"metadata":{"id":"8PEmAIVpIiH1"}},{"cell_type":"code","source":["data_neu = next(iter(train_data_loader_neu))\n","\n","model_neu = PersonalityClassifier(2)  #2 is number of classes (binary classifier for a personality)\n","model_neu = model_neu.to(device)\n","\n","input_ids = data_neu['input_ids'].to(device)\n","attention_mask = data_neu['attention_mask'].to(device)"],"metadata":{"id":"Ov8RINB0Iggy","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["4d0add3483b5489a99a3773e061a9456","25f7baa040aa47b28e75dbac21b9be25","da51681035f644608c11ad180a491325","e36a5b6707014804815a5ff86b2500f4","e6101603eed54247984260106677b393","42a38114d30747d28f19071a6d163cb7","a4e75aeba33c45378b326ac11d9baac6","5399c1fcd87f416793abf8c32f484599","913fbd3db5044595950da737acdf9e07","9982eed0fb994719a7346fe1a0a746f1","613a396aac2443518f4fcf9ec02f05f8"]},"executionInfo":{"status":"ok","timestamp":1685779176923,"user_tz":-120,"elapsed":8848,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"e8d7f7f5-2e89-4ba3-ca3c-c71ee26132f1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0add3483b5489a99a3773e061a9456"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["##### Agreeableness"],"metadata":{"id":"lUFEWy9_I3v1"}},{"cell_type":"code","source":["data_agr = next(iter(train_data_loader_agr))\n","\n","model_agr = PersonalityClassifier(2)  #2 is number of classes (binary classifier for a personality)\n","model_agr = model_agr.to(device)\n","\n","input_ids = data_agr['input_ids'].to(device)\n","attention_mask = data_agr['attention_mask'].to(device)"],"metadata":{"id":"8srYlMf3IgOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Consciousness"],"metadata":{"id":"wJNBusK0JKNY"}},{"cell_type":"code","source":["data_con = next(iter(train_data_loader_con))\n","\n","model_con = PersonalityClassifier(2)  #2 is number of classes (binary classifier for a personality)\n","model_con = model_con.to(device)\n","\n","input_ids = data_con['input_ids'].to(device)\n","attention_mask = data_con['attention_mask'].to(device)"],"metadata":{"id":"XApbZGfwIffv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Openness"],"metadata":{"id":"Q1TvX_yOJcKo"}},{"cell_type":"code","source":["data_opn = next(iter(train_data_loader_opn))\n","\n","model_opn = PersonalityClassifier(2)  #2 is number of classes (binary classifier for a personality)\n","model_opn = model_opn.to(device)\n","\n","input_ids = data_opn['input_ids'].to(device)\n","attention_mask = data_opn['attention_mask'].to(device)"],"metadata":{"id":"RBIJcidOIfRn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"oac4lifYlMNR"}},{"cell_type":"code","source":["def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","    ):\n","    model = model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        targets = d[\"targets\"].to(device)\n","\n","        outputs = model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","\n","        correct_predictions += torch.sum(preds == targets)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","\n","            outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            loss = loss_fn(outputs, targets)\n","\n","            correct_predictions += torch.sum(preds == targets)\n","            losses.append(loss.item())\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def get_predictions(model, data_loader):\n","    model = model.eval()\n","\n","    review_texts = []\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            texts = d[\"doc_text\"]\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","\n","            outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            probs = F.softmax(outputs, dim=1)\n","\n","            review_texts.extend(texts)\n","            predictions.extend(preds)\n","            prediction_probs.extend(probs)\n","            real_values.extend(targets)\n","\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    return review_texts, predictions, prediction_probs, real_values\n","\n","def save_model(model, name):\n","  path_pt = F\"/content/gdrive/My Drive/Colab Notebooks/models/{name}.pt\"\n","  path_bin = F\"/content/gdrive/My Drive/Colab Notebooks/models/{name}.bin\" \n","  torch.save(model.state_dict(), path_pt)\n","  torch.save(model.state_dict(), path_bin)"],"metadata":{"id":"nmvmu49rlk69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 10"],"metadata":{"id":"HIu346w-J2oC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_transformer_training(model, train_data_loader, val_data_loader, test_data_loader, personality):  \n","  optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","  total_steps = len(train_data_loader) * EPOCHS\n","\n","  scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n","  )\n","\n","  loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","\n","  history = defaultdict(list)\n","  best_accuracy = 0\n","\n","  for epoch in range(EPOCHS):\n","\n","      print(f'Epoch {epoch + 1}/{EPOCHS}')\n","      print('-' * 10)\n","\n","      train_acc, train_loss = train_epoch(\n","      model,\n","      train_data_loader,    \n","      loss_fn, \n","      optimizer, \n","      device, \n","      scheduler, \n","      len(df_train)\n","      )\n","\n","      print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","      val_acc, val_loss = eval_model(\n","      model,\n","      val_data_loader,\n","      loss_fn, \n","      device, \n","      len(df_val)\n","      )\n","\n","      print(f'Val   loss {val_loss} accuracy {val_acc}')\n","      print()\n","\n","      history['train_acc'].append(train_acc)\n","      history['train_loss'].append(train_loss)\n","      history['val_acc'].append(val_acc)\n","      history['val_loss'].append(val_loss)\n","\n","      if val_acc > best_accuracy:\n","          save_model(model, 'model_'+personality)\n","          best_accuracy = val_acc\n","\n","  test_acc, _ = eval_model(\n","        model,\n","        test_data_loader,\n","        loss_fn,\n","        device,\n","        len(df_test)\n","      )\n","  print('\\nTest Accuracy:\\n')\n","  print(test_acc.item())\n","\n","  y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  model,\n","  test_data_loader\n","  )\n","\n","  f1 = f1_score(y_test, y_pred)\n","  print(\"F1 score:\", f1)\n","  print(classification_report(y_test, y_pred, target_names=['no', 'yes']))"],"metadata":{"id":"-TW6-2Euk9Da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"QabL0w7-pciA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Extraversion"],"metadata":{"id":"wlOxByGyJvyC"}},{"cell_type":"code","source":["my_transformer_training(model_ext, train_data_loader_ext, val_data_loader_ext, test_data_loader_ext, 'ext')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMt8rUIMNWaH","outputId":"56ddbfae-5879-4153-e378-4c98f75d1bc1","executionInfo":{"status":"ok","timestamp":1685757147640,"user_tz":-120,"elapsed":5043746,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.7092646113299331 accuracy 0.5157933853586028\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.7062871409089941 accuracy 0.5016722408026756\n","\n","Epoch 2/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.664212374506521 accuracy 0.6020066889632107\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.6627673130286368 accuracy 0.6153846153846154\n","\n","Epoch 3/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.46934673476847993 accuracy 0.7841880341880342\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.7668755007417578 accuracy 0.6739130434782609\n","\n","Epoch 4/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.2567425285802288 accuracy 0.9055183946488294\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.223515646238076 accuracy 0.6722408026755853\n","\n","Epoch 5/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.16531836522973514 accuracy 0.9564288368636195\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.9641587357772023 accuracy 0.657190635451505\n","\n","Epoch 6/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.10707309082997021 accuracy 0.974823485693051\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.902530126665768 accuracy 0.6939799331103679\n","\n","Epoch 7/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.07465383365792606 accuracy 0.9823485693050911\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.9039643642149473 accuracy 0.7140468227424749\n","\n","Epoch 8/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.049346691313071955 accuracy 0.9897807506503159\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 2.1390673627978876 accuracy 0.6923076923076923\n","\n","Epoch 9/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.04148940235772438 accuracy 0.9910813823857303\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 2.166257117923937 accuracy 0.697324414715719\n","\n","Epoch 10/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.02571399618400162 accuracy 0.9949832775919732\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 2.217042054000654 accuracy 0.7023411371237458\n","\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Accuracy:\n","\n","0.705685618729097\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["F1 score: 0.7105263157894737\n","              precision    recall  f1-score   support\n","\n","          no       0.73      0.67      0.70       306\n","         yes       0.68      0.74      0.71       292\n","\n","    accuracy                           0.71       598\n","   macro avg       0.71      0.71      0.71       598\n","weighted avg       0.71      0.71      0.71       598\n","\n"]}]},{"cell_type":"markdown","source":["##### Neuroticism"],"metadata":{"id":"938cSjREK4MT"}},{"cell_type":"code","source":["my_transformer_training(model_neu, train_data_loader_neu, val_data_loader_neu, test_data_loader_neu, 'neu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685784157021,"user_tz":-120,"elapsed":4949421,"user":{"displayName":"Ivan Linardić","userId":"11390504310880094963"}},"outputId":"d8f22743-5cb6-43cd-a8e5-47fe8f944cf4","id":"wPnQcxNlK1cR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n","Train loss 0.7110793531206706 accuracy 0.510776662950576\n","Val   loss 0.7029258404907427 accuracy 0.4899665551839465\n","\n","Epoch 2/10\n","----------\n","Train loss 0.701769827202206 accuracy 0.5030657748049052\n","Val   loss 0.6933733303295938 accuracy 0.4899665551839465\n","\n","Epoch 3/10\n","----------\n","Train loss 0.7221964618072113 accuracy 0.4982348569305091\n","Val   loss 0.6950547742216211 accuracy 0.4899665551839465\n","\n","Epoch 4/10\n","----------\n","Train loss 0.7146121616880748 accuracy 0.5078966926793014\n","Val   loss 0.6960941098238292 accuracy 0.4899665551839465\n","\n","Epoch 5/10\n","----------\n","Train loss 0.7094054312443839 accuracy 0.5052954292084727\n","Val   loss 0.6974547693603917 accuracy 0.4899665551839465\n","\n","Epoch 6/10\n","----------\n","Train loss 0.7000314820043045 accuracy 0.5207172054998143\n","Val   loss 0.6836582437941903 accuracy 0.5484949832775919\n","\n","Epoch 7/10\n","----------\n","Train loss 0.6756480939320966 accuracy 0.5906726124117428\n","Val   loss 0.7005111904520738 accuracy 0.5501672240802675\n","\n","Epoch 8/10\n","----------\n","Train loss 0.6327457742829202 accuracy 0.6540319583797845\n","Val   loss 0.6848320780616057 accuracy 0.5986622073578595\n","\n","Epoch 9/10\n","----------\n","Train loss 0.5658404836417308 accuracy 0.7189706428836864\n","Val   loss 0.7253302625919643 accuracy 0.6120401337792643\n","\n","Epoch 10/10\n","----------\n","Train loss 0.5105425455063626 accuracy 0.7653288740245262\n","Val   loss 0.7228394903634724 accuracy 0.6103678929765887\n","\n","\n","Test Accuracy:\n","\n","0.580267558528428\n","F1 score: 0.5795644891122279\n","              precision    recall  f1-score   support\n","\n","          no       0.61      0.55      0.58       315\n","         yes       0.55      0.61      0.58       283\n","\n","    accuracy                           0.58       598\n","   macro avg       0.58      0.58      0.58       598\n","weighted avg       0.58      0.58      0.58       598\n","\n"]}]},{"cell_type":"markdown","source":["##### Agreeableness"],"metadata":{"id":"MyqKJE5HK_N2"}},{"cell_type":"code","source":["my_transformer_training(model_agr, train_data_loader_agr, val_data_loader_agr, test_data_loader_agr, 'agr')"],"metadata":{"id":"Vbk9Jn4RNlOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Consciousness"],"metadata":{"id":"P0fTwot1K_lr"}},{"cell_type":"code","source":["my_transformer_training(model_con, train_data_loader_con, val_data_loader_con, test_data_loader_con, 'con')"],"metadata":{"id":"5TcKGqTYNmY4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Openness"],"metadata":{"id":"WU-kPUTQLAHb"}},{"cell_type":"code","source":["my_transformer_training(model_opn, train_data_loader_opn, val_data_loader_opn, test_data_loader_opn, 'opn')"],"metadata":{"id":"-navRM-mNm8B"},"execution_count":null,"outputs":[]}]}