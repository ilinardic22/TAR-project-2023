{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>average_word_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', 'right', 'now', 'i', 'just', 'woke', ...</td>\n",
       "      <td>['well, right now i just woke up from a mid-da...</td>\n",
       "      <td>[('well', 'right'), ('right', 'now'), ('now', ...</td>\n",
       "      <td>[('well', 'right', 'now'), ('right', 'now', 'i...</td>\n",
       "      <td>[ 1.46904569e-02  1.52049020e-01 -2.17639774e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', 'here', 'we', 'go', 'with', 'the', 's...</td>\n",
       "      <td>['well, here we go with the stream of consciou...</td>\n",
       "      <td>[('well', 'here'), ('here', 'we'), ('we', 'go'...</td>\n",
       "      <td>[('well', 'here', 'we'), ('here', 'we', 'go'),...</td>\n",
       "      <td>[ 1.93020366e-02  2.00337350e-01 -2.47012377e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['an', 'open', 'keyboard', 'and', 'buttons', '...</td>\n",
       "      <td>['an open keyboard and buttons to push.', 'the...</td>\n",
       "      <td>[('an', 'open'), ('open', 'keyboard'), ('keybo...</td>\n",
       "      <td>[('an', 'open', 'keyboard'), ('open', 'keyboar...</td>\n",
       "      <td>[ 1.21683925e-02  1.49960428e-01 -2.17856288e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['i', 'cant', 'believe', 'it', 'its', 'really'...</td>\n",
       "      <td>[\"i can't believe it!\", \"it's really happening...</td>\n",
       "      <td>[('i', 'cant'), ('cant', 'believe'), ('believe...</td>\n",
       "      <td>[('i', 'cant', 'believe'), ('cant', 'believe',...</td>\n",
       "      <td>[-1.21900747e-02  1.94802403e-01 -2.04183444e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', 'here', 'i', 'go', 'with', 'the', 'go...</td>\n",
       "      <td>['well, here i go with the good old stream of ...</td>\n",
       "      <td>[('well', 'here'), ('here', 'i'), ('i', 'go'),...</td>\n",
       "      <td>[('well', 'here', 'i'), ('here', 'i', 'go'), (...</td>\n",
       "      <td>[-6.53621508e-03  1.72239631e-01 -2.12745324e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  cEXT  cNEU  cAGR  cCON  \\\n",
       "0  Well, right now I just woke up from a mid-day ...     0     1     1     0   \n",
       "1  Well, here we go with the stream of consciousn...     0     0     1     0   \n",
       "2  An open keyboard and buttons to push. The thin...     0     1     0     1   \n",
       "3  I can't believe it!  It's really happening!  M...     1     0     1     1   \n",
       "4  Well, here I go with the good old stream of co...     1     0     1     0   \n",
       "\n",
       "   cOPN                                              words  \\\n",
       "0     1  ['well', 'right', 'now', 'i', 'just', 'woke', ...   \n",
       "1     0  ['well', 'here', 'we', 'go', 'with', 'the', 's...   \n",
       "2     1  ['an', 'open', 'keyboard', 'and', 'buttons', '...   \n",
       "3     0  ['i', 'cant', 'believe', 'it', 'its', 'really'...   \n",
       "4     1  ['well', 'here', 'i', 'go', 'with', 'the', 'go...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  ['well, right now i just woke up from a mid-da...   \n",
       "1  ['well, here we go with the stream of consciou...   \n",
       "2  ['an open keyboard and buttons to push.', 'the...   \n",
       "3  [\"i can't believe it!\", \"it's really happening...   \n",
       "4  ['well, here i go with the good old stream of ...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [('well', 'right'), ('right', 'now'), ('now', ...   \n",
       "1  [('well', 'here'), ('here', 'we'), ('we', 'go'...   \n",
       "2  [('an', 'open'), ('open', 'keyboard'), ('keybo...   \n",
       "3  [('i', 'cant'), ('cant', 'believe'), ('believe...   \n",
       "4  [('well', 'here'), ('here', 'i'), ('i', 'go'),...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [('well', 'right', 'now'), ('right', 'now', 'i...   \n",
       "1  [('well', 'here', 'we'), ('here', 'we', 'go'),...   \n",
       "2  [('an', 'open', 'keyboard'), ('open', 'keyboar...   \n",
       "3  [('i', 'cant', 'believe'), ('cant', 'believe',...   \n",
       "4  [('well', 'here', 'i'), ('here', 'i', 'go'), (...   \n",
       "\n",
       "                              average_word_embedding  \n",
       "0  [ 1.46904569e-02  1.52049020e-01 -2.17639774e-...  \n",
       "1  [ 1.93020366e-02  2.00337350e-01 -2.47012377e-...  \n",
       "2  [ 1.21683925e-02  1.49960428e-01 -2.17856288e-...  \n",
       "3  [-1.21900747e-02  1.94802403e-01 -2.04183444e-...  \n",
       "4  [-6.53621508e-03  1.72239631e-01 -2.12745324e-...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "essays=pd.read_csv(\"../data/essays_expanded.csv\")\n",
    "\n",
    "essays.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline classifier: random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_random_classifier(df_test, personality):\n",
    "\n",
    "    y_test = df_test[[personality]]\n",
    "    y_pred = np.random.randint(2, size=len(y_test))\n",
    "    print(classification_report(y_pred=y_pred, y_true=y_test))\n",
    "    return f1_score(y_pred=y_pred, y_true=y_test, average=\"macro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR and SVM training and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(df_train, vectorizer, personality, lr_kwargs={\"solver\": \"liblinear\"}):\n",
    "    \"\"\"\n",
    "    Receives the train set `df_train` as pd.DataFrame and extracts lemma n-grams\n",
    "    with their correspoding labels (news type).\n",
    "    The text is vectorized and used to train a logistic regression with\n",
    "    training arguments passed as `lr_kwargs`.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    vectorizer.set_params(max_df=df_train.shape[0])\n",
    "    X=vectorizer.fit_transform(df_train.TEXT)\n",
    "    model=LR(**lr_kwargs)\n",
    "    model.fit(X, df_train[[personality]])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_svm(df_train, vectorizer, personality):\n",
    "    \"\"\"\n",
    "    Receives the train set `df_train` as pd.DataFrame and extracts lemma n-grams\n",
    "    with their correspoding labels (news type).\n",
    "    The text is vectorized and used to train a logistic regression with\n",
    "    training arguments passed as `lr_kwargs`.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    vectorizer.set_params(max_df=df_train.shape[0])\n",
    "    X=vectorizer.fit_transform(df_train.TEXT)\n",
    "    model=SVC(kernel=\"linear\")\n",
    "    model.fit(X, df_train[[personality]])\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_performance(model, df_test, vectorizer, personality):\n",
    "\n",
    "    X_test, y_test = df_test.TEXT, df_test[[personality]]\n",
    "    X_vec = vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(X_vec)\n",
    "    print(classification_report(y_pred=y_pred, y_true=y_test))\n",
    "    return f1_score(y_pred=y_pred, y_true=y_test, average=\"macro\")\n",
    "\n",
    "\n",
    "def influential_ngrams(model, vectorizer, is_lr=True):\n",
    "    \"\"\"\n",
    "    Receives a model (LR or SVM) and a vectorizer.\n",
    "    Prints the most influential n-grams.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_lr:\n",
    "        print(\"Logistic regression\\n\")\n",
    "        print(\"The most influential n-grams for classification 1 are:\")\n",
    "        ind = np.argsort(model.coef_)[0][-10:]\n",
    "        for index in ind:\n",
    "            print(vectorizer.get_feature_names()[index])\n",
    "\n",
    "\n",
    "        print(\"The most influential n-grams for classification 0 are:\")\n",
    "        ind = np.argsort(model.coef_)[0][:10]\n",
    "        for index in ind:\n",
    "            print(vectorizer.get_feature_names()[index])\n",
    "\n",
    "    else:\n",
    "        print(\"SVM\\n\")\n",
    "        print(\"The most influential n-grams for classification 1 are:\")\n",
    "        ind = np.argsort(svm.coef_.toarray())[0][-10:]\n",
    "        for index in ind:\n",
    "            print(count_vectorizer.get_feature_names()[index])\n",
    "\n",
    "        print(\"The most influential n-grams for classification 0 are:\")\n",
    "        ind = np.argsort(svm.coef_.toarray())[0][:10]\n",
    "        for index in ind:\n",
    "            print(count_vectorizer.get_feature_names()[index])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Raw text n-grams\n",
    "\n",
    "\n",
    "The n-grams will be extracted out of the raw essay text and given to the models as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_train, essays_test = train_test_split(\n",
    "    essays[[\"TEXT\", \"cEXT\", \"cOPN\", \"cAGR\", \"cCON\", \"cNEU\"]], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "count_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurin\\anaconda3\\envs\\tarlab1\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.48      0.49       227\n",
      "           1       0.57      0.60      0.59       267\n",
      "\n",
      "    accuracy                           0.54       494\n",
      "   macro avg       0.54      0.54      0.54       494\n",
      "weighted avg       0.54      0.54      0.54       494\n",
      "\n",
      "f1 = 0.539\n"
     ]
    }
   ],
   "source": [
    "lr = train_lr(essays_train, count_vectorizer, \"cEXT\")\n",
    "f1 = test_performance(lr, essays_test, count_vectorizer, \"cEXT\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "is\n",
      "its\n",
      "sorority\n",
      "and\n",
      "love\n",
      "boyfriend\n",
      "fun\n",
      "all\n",
      "am\n",
      "so\n",
      "The most influential n-grams for classification 0 are:\n",
      "don\n",
      "there\n",
      "in\n",
      "should\n",
      "want\n",
      "something\n",
      "eyes\n",
      "perhaps\n",
      "very\n",
      "mother\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(lr, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49       227\n",
      "           1       0.58      0.62      0.60       267\n",
      "\n",
      "    accuracy                           0.55       494\n",
      "   macro avg       0.55      0.54      0.54       494\n",
      "weighted avg       0.55      0.55      0.55       494\n",
      "\n",
      "f1 = 0.544\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(essays_train, count_vectorizer, \"cEXT\")\n",
    "f1 = test_performance(svm, essays_test, count_vectorizer, \"cEXT\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "ready\n",
      "mean\n",
      "if\n",
      "love\n",
      "its\n",
      "all\n",
      "am\n",
      "boyfriend\n",
      "fun\n",
      "so\n",
      "The most influential n-grams for classification 0 are:\n",
      "don\n",
      "should\n",
      "in\n",
      "something\n",
      "there\n",
      "want\n",
      "eyes\n",
      "very\n",
      "perhaps\n",
      "real\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(svm, count_vectorizer, is_lr=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58       237\n",
      "           1       0.62      0.60      0.61       257\n",
      "\n",
      "    accuracy                           0.60       494\n",
      "   macro avg       0.60      0.60      0.60       494\n",
      "weighted avg       0.60      0.60      0.60       494\n",
      "\n",
      "f1 = 0.597\n"
     ]
    }
   ],
   "source": [
    "lr = train_lr(essays_train, count_vectorizer, \"cOPN\")\n",
    "f1 = test_performance(lr, essays_test, count_vectorizer, \"cOPN\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "like\n",
      "cat\n",
      "ll\n",
      "too\n",
      "maybe\n",
      "re\n",
      "love\n",
      "music\n",
      "of\n",
      "you\n",
      "The most influential n-grams for classification 0 are:\n",
      "college\n",
      "is\n",
      "to\n",
      "my\n",
      "because\n",
      "school\n",
      "home\n",
      "class\n",
      "have\n",
      "classes\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(lr, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.59       237\n",
      "           1       0.61      0.56      0.58       257\n",
      "\n",
      "    accuracy                           0.59       494\n",
      "   macro avg       0.59      0.59      0.59       494\n",
      "weighted avg       0.59      0.59      0.58       494\n",
      "\n",
      "f1 = 0.585\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(essays_train, count_vectorizer, \"cOPN\")\n",
    "f1 = test_performance(svm, essays_test, count_vectorizer, \"cOPN\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "crazy\n",
      "love\n",
      "like\n",
      "maybe\n",
      "you\n",
      "ll\n",
      "cat\n",
      "of\n",
      "re\n",
      "music\n",
      "The most influential n-grams for classification 0 are:\n",
      "college\n",
      "because\n",
      "is\n",
      "boyfriend\n",
      "assignment\n",
      "class\n",
      "home\n",
      "tomorrow\n",
      "game\n",
      "confused\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(svm, count_vectorizer, is_lr=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agreeableness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.38      0.42       220\n",
      "           1       0.58      0.68      0.62       274\n",
      "\n",
      "    accuracy                           0.54       494\n",
      "   macro avg       0.53      0.53      0.52       494\n",
      "weighted avg       0.54      0.54      0.53       494\n",
      "\n",
      "f1 = 0.524\n"
     ]
    }
   ],
   "source": [
    "lr = train_lr(essays_train, count_vectorizer, \"cAGR\")\n",
    "f1 = test_performance(lr, essays_test, count_vectorizer, \"cAGR\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "least\n",
      "would\n",
      "with\n",
      "right\n",
      "on\n",
      "to\n",
      "really\n",
      "so\n",
      "family\n",
      "have\n",
      "The most influential n-grams for classification 0 are:\n",
      "stupid\n",
      "girlfriend\n",
      "don\n",
      "is\n",
      "damn\n",
      "read\n",
      "more\n",
      "nothing\n",
      "same\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(lr, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.38      0.42       220\n",
      "           1       0.57      0.65      0.61       274\n",
      "\n",
      "    accuracy                           0.53       494\n",
      "   macro avg       0.52      0.52      0.51       494\n",
      "weighted avg       0.52      0.53      0.52       494\n",
      "\n",
      "f1 = 0.514\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(essays_train, count_vectorizer, \"cAGR\")\n",
    "f1 = test_performance(svm, essays_test, count_vectorizer, \"cAGR\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "many\n",
      "worried\n",
      "would\n",
      "least\n",
      "so\n",
      "right\n",
      "family\n",
      "on\n",
      "with\n",
      "have\n",
      "The most influential n-grams for classification 0 are:\n",
      "stupid\n",
      "girlfriend\n",
      "is\n",
      "read\n",
      "damn\n",
      "store\n",
      "same\n",
      "don\n",
      "nothing\n",
      "wont\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(svm, count_vectorizer, is_lr=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conscientiousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51       227\n",
      "           1       0.58      0.57      0.57       267\n",
      "\n",
      "    accuracy                           0.54       494\n",
      "   macro avg       0.54      0.54      0.54       494\n",
      "weighted avg       0.54      0.54      0.54       494\n",
      "\n",
      "f1 = 0.540\n"
     ]
    }
   ],
   "source": [
    "lr = train_lr(essays_train, count_vectorizer, \"cCON\")\n",
    "f1 = test_performance(lr, essays_test, count_vectorizer, \"cCON\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "today\n",
      "it\n",
      "tonight\n",
      "hope\n",
      "party\n",
      "and\n",
      "the\n",
      "my\n",
      "he\n",
      "to\n",
      "The most influential n-grams for classification 0 are:\n",
      "want\n",
      "hate\n",
      "don\n",
      "this\n",
      "think\n",
      "re\n",
      "wake\n",
      "god\n",
      "point\n",
      "chance\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(lr, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.53       227\n",
      "           1       0.59      0.57      0.58       267\n",
      "\n",
      "    accuracy                           0.56       494\n",
      "   macro avg       0.56      0.56      0.56       494\n",
      "weighted avg       0.56      0.56      0.56       494\n",
      "\n",
      "f1 = 0.555\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(essays_train, count_vectorizer, \"cCON\")\n",
    "f1 = test_performance(svm, essays_test, count_vectorizer, \"cCON\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "student\n",
      "decision\n",
      "today\n",
      "couldn\n",
      "tonight\n",
      "my\n",
      "hope\n",
      "to\n",
      "he\n",
      "party\n",
      "The most influential n-grams for classification 0 are:\n",
      "want\n",
      "hate\n",
      "this\n",
      "wake\n",
      "point\n",
      "re\n",
      "think\n",
      "chance\n",
      "don\n",
      "music\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(svm, count_vectorizer, is_lr=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuroticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61       260\n",
      "           1       0.58      0.63      0.60       234\n",
      "\n",
      "    accuracy                           0.61       494\n",
      "   macro avg       0.61      0.61      0.61       494\n",
      "weighted avg       0.61      0.61      0.61       494\n",
      "\n",
      "f1 = 0.605\n"
     ]
    }
   ],
   "source": [
    "lr = train_lr(essays_train, count_vectorizer, \"cNEU\")\n",
    "f1 = test_performance(lr, essays_test, count_vectorizer, \"cNEU\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "don\n",
      "feel\n",
      "everything\n",
      "want\n",
      "scared\n",
      "money\n",
      "me\n",
      "sex\n",
      "life\n",
      "stressed\n",
      "The most influential n-grams for classification 0 are:\n",
      "its\n",
      "would\n",
      "her\n",
      "many\n",
      "semester\n",
      "already\n",
      "beat\n",
      "mind\n",
      "as\n",
      "texas\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(lr, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.60       260\n",
      "           1       0.56      0.62      0.59       234\n",
      "\n",
      "    accuracy                           0.59       494\n",
      "   macro avg       0.59      0.59      0.59       494\n",
      "weighted avg       0.60      0.59      0.59       494\n",
      "\n",
      "f1 = 0.593\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(essays_train, count_vectorizer, \"cNEU\")\n",
    "f1 = test_performance(svm, essays_test, count_vectorizer, \"cNEU\")\n",
    "print(f\"f1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "\n",
      "The most influential n-grams for classification 1 are:\n",
      "worry\n",
      "this\n",
      "scared\n",
      "me\n",
      "everything\n",
      "life\n",
      "boyfriend\n",
      "sex\n",
      "money\n",
      "stressed\n",
      "The most influential n-grams for classification 0 are:\n",
      "its\n",
      "many\n",
      "her\n",
      "would\n",
      "already\n",
      "pledge\n",
      "semester\n",
      "beat\n",
      "glad\n",
      "mind\n"
     ]
    }
   ],
   "source": [
    "influential_ngrams(svm, count_vectorizer, is_lr=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarlab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
